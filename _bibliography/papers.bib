---
---

@inproceedings{sargood2025cocolit,
   author    = {Sargood<sup>‚Ä†</sup>, Alec and Puglisi<sup>‚Ä†</sup>, Lemuel and Cole, James H and Oxtoby, Neil P and Rav√¨, Daniele and Alexander, Daniel C},
   title     = {CoCoLIT: ControlNet-Conditioned Latent Image Translation for MRI to Amyloid PET Synthesis},
   booktitle = {AAAI},
   year      = {2026},
   selected  = {true},
   preview   = {CoCoLIT-preview.gif},
   pdf       = {https://arxiv.org/pdf/2508.01292},
   code      = {https://github.com/brAIn-science/CoCoLIT},
   abstract  = {Synthesizing amyloid PET scans from the more widely available and accessible structural MRI modality offers a promising, cost-effective approach for large-scale Alzheimer's Disease (AD) screening. This is motivated by evidence that, while MRI does not directly detect amyloid pathology, it may nonetheless encode information correlated with amyloid deposition that can be uncovered through advanced modeling. However, the high dimensionality and structural complexity of 3D neuroimaging data pose significant challenges for existing MRI-to-PET translation methods. Modeling the cross-modality relationship in a lower-dimensional latent space can simplify the learning task and enable more effective translation. As such, we present CoCoLIT (ControlNet-Conditioned Latent Image Translation), a diffusion-based latent generative framework that incorporates three main innovations: (1) a novel Weighted Image Space Loss (WISL) that improves latent representation learning and synthesis quality; (2) a theoretical and empirical analysis of Latent Average Stabilization (LAS), an existing technique used in similar generative models to enhance inference consistency; and (3) the introduction of ControlNet-based conditioning for MRI-to-PET translation. We evaluate CoCoLIT's performance on publicly available datasets and find that our model significantly outperforms state-of-the-art methods on both image-based and amyloid-related metrics. Notably, in amyloid-positivity classification, CoCoLIT outperforms the second-best method with improvements of +10.5\% on the internal dataset and +23.7\% on the external dataset.},
   honor     = {<sup>‚Ä†</sup> Joint first authorship}
}

@inproceedings{scardace2025novel,
   author    = {Scardace<sup>‚Ä†</sup>, Antonio and Puglisi<sup>‚Ä†</sup>, Lemuel and Guarnera, Francesco and Battiato, Sebastiano  and Rav√¨, Daniele},
   title     = {A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis},
   booktitle = {WACV},
   year      = {2026},
   selected  = {true},
   preview   = {DeepSSIM.png},
   pdf       = {https://arxiv.org/pdf/2509.16582},
   code      = {https://github.com/brAIn-science/DeepSSIM},
   abstract  = {Deep generative models have emerged as a transformative tool in medical imaging, offering substantial potential for synthetic data generation. However, recent empirical studies highlight a critical vulnerability: these models can memorize sensitive training data, posing significant risks of unauthorized patient information disclosure. Detecting memorization in generative models remains particularly challenging, necessitating scalable methods capable of identifying training data leakage across large sets of generated samples. In this work, we propose DeepSSIM, a novel self-supervised metric for quantifying memorization in generative models. DeepSSIM is trained to: i) project images into a learned embedding space and ii) force the cosine similarity between embeddings to match the ground-truth SSIM (Structural Similarity Index) scores computed in the image space. To capture domain-specific anatomical features, training incorporates structure-preserving augmentations, allowing DeepSSIM to estimate similarity reliably without requiring precise spatial alignment. We evaluate DeepSSIM in a case study involving synthetic brain MRI data generated by a Latent Diffusion Model (LDM) trained under memorization-prone conditions, using 2,195 MRI scans from two publicly available datasets (IXI and CoRR). Compared to state-of-the-art memorization metrics, DeepSSIM achieves superior performance, improving F1 scores by an average of +52.03% over the best existing method. Code and data of our approach are publicly available at the following link: https://github.com/brAIn-science/DeepSSIM.},
   honor     = {<sup>‚Ä†</sup> Joint first authorship}
}

@inproceedings{puglisi2025brain,
   author    = {Puglisi, Lemuel and Alexander, Daniel C and Rav√¨, Daniele},
   title     = {Brain Latent Progression: Individual-based Spatiotemporal Disease Progression on 3D Brain MRIs via Latent Diffusion},
   booktitle = {Medical Image Analysis},
   year      = {2025},
   selected  = {true},
   preview   = {brlp-plot-2.gif},
   pdf       = {https://www.sciencedirect.com/science/article/pii/S1361841525002816},
   code      = {https://github.com/LemuelPuglisi/BrLP},
   abstract  = {The growing availability of longitudinal Magnetic Resonance Imaging (MRI) datasets has facilitated Artificial Intelligence (AI)-driven modeling of disease progression, making it possible to predict future medical scans for individual patients. However, despite significant advancements in AI, current methods continue to face challenges including achieving patient-specific individualization, ensuring spatiotemporal consistency, efficiently utilizing longitudinal data, and managing the substantial memory demands of 3D scans. To address these challenges, we propose Brain Latent Progression (BrLP), a novel spatiotemporal model designed to predict individual-level disease progression in 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates in a small latent space, mitigating the computational challenges posed by high-dimensional imaging data; (ii) it explicitly integrates subject metadata to enhance the individualization of predictions; (iii) it incorporates prior knowledge of disease dynamics through an auxiliary model, facilitating the integration of longitudinal data; and (iv) it introduces the Latent Average Stabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in the predicted progression at inference time and (b) allows us to derive a measure of the uncertainty for the prediction at the global and voxel level. We train and evaluate BrLP on 11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its generalizability on an external test set comprising 2,257 MRIs from 962 subjects. Our experiments compare BrLP-generated MRI scans with real follow-up MRIs, demonstrating state-of-the-art accuracy compared to existing methods. The code is publicly available at: https://github.com/LemuelPuglisi/BrLP.},
   honor     = {<i>Impact Factor = 11.8</i>}
}

@inproceedings{puglisi2024synthba,
   author    = {Puglisi, Lemuel and Rondinella, Alessia and De Meo, Linda and Guarnera, Francesco and Battiato, Sebastiano and Rav√¨, Daniele},
   title     = {SynthBA: Reliable Brain Age Estimation Across Multiple MRI Sequences and Resolutions},
   booktitle = {IEEE Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering},
   year      = {2024},
   selected  = {true},
   comment   = {MetroXRAINE 2024},
   preview   = {synthba-plot.png},
   pdf       = {https://arxiv.org/pdf/2406.00365},
   code      = {https://github.com/LemuelPuglisi/SynthBA},
   abstract  = {Brain age is a critical measure that reflects the biological ageing process of the brain. The gap between brain age and chronological age, referred to as brain PAD (Predicted Age Difference), has been utilized to investigate neurodegenerative conditions. Brain age can be predicted using MRIs and machine learning techniques. However, existing methods are often sensitive to acquisition-related variabilities, such as differences in acquisition protocols, scanners, MRI sequences, and resolutions, significantly limiting their application in highly heterogeneous clinical settings. In this study, we introduce Synthetic Brain Age (SynthBA), a robust deep-learning model designed for predicting brain age. SynthBA utilizes an advanced domain randomization technique, ensuring effective operation across a wide array of acquisition-related variabilities. To assess the effectiveness and robustness of SynthBA, we evaluate its predictive capabilities on internal and external datasets, encompassing various MRI sequences and resolutions, and compare it with state-of-the-art techniques. Additionally, we calculate the brain PAD in a large cohort of subjects with Alzheimer‚Äôs Disease (AD), demonstrating a significant correlation with AD-related measures of cognitive dysfunction. SynthBA holds the potential to facilitate the broader adoption of brain age prediction in clinical settings, where re-training or fine-tuning is often unfeasible. The SynthBA source code and pre-trained models are publicly available at https://github.com/LemuelPuglisi/SynthBA.},
}

@inproceedings{puglisi2024enhancing,
   author    = {Puglisi, Lemuel and Alexander, Daniel C and Rav√¨, Daniele},
   title     = {Enhancing Spatiotemporal Disease Progression Models via Latent Diffusion and Prior Knowledge},
   booktitle = {International Conference on Medical Image Computing and Computer Assisted Intervention},
   year      = {2024},
   selected  = {true},
   comment   = {MICCAI 2024},
   preview   = {brlp-plot-2.gif},
   pdf       = {https://arxiv.org/pdf/2405.03328},
   code      = {https://github.com/LemuelPuglisi/BrLP},
   abstract  = {In this work, we introduce Brain Latent Progression (BrLP), a novel spatiotemporal disease progression model based on latent diffusion. BrLP is designed to predict the evolution of diseases at the individual level on 3D brain MRIs. Existing deep generative models developed for this task are primarily data-driven and face challenges in learning disease progressions. BrLP addresses these challenges by incorporating prior knowledge from disease models to enhance the accuracy of predictions. To implement this, we propose to integrate an auxiliary model that infers volumetric changes in various brain regions. Additionally, we introduce Latent Average Stabilization (LAS), a novel technique to improve spatiotemporal consistency of the predicted progression. BrLP is trained and evaluated on a large dataset comprising 11,730 T1-weighted brain MRIs from 2,805 subjects, collected from three publicly available, longitudinal Alzheimer's Disease (AD) studies. In our experiments, we compare the MRI scans generated by BrLP with the actual follow-up MRIs available from the subjects, in both cross-sectional and longitudinal settings. BrLP demonstrates significant improvements over existing methods, with an increase of 22\% in volumetric accuracy across AD-related brain regions and 43\% in image similarity to the ground-truth scans. The ability of BrLP to generate conditioned 3D scans at the subject level, along with the novelty of integrating prior knowledge to enhance accuracy, represents a significant advancement in disease progression modeling, opening new avenues for precision medicine. The code of BrLP is available at the following link: https://github.com/LemuelPuglisi/BrLP.},
   demo      = {https://youtu.be/6YKz2MNM4jg?si=gE_4Q_cDMzaOTReE},
   honor     = {üåü Oral, Best Paper Award Candidate (top <1%)}
}

@inproceedings{puglisi2023deepbrainprint,
   author    = {Puglisi, Lemuel and Eshaghi, Arman and Parker, Geoff and Barkhof, Frederik and Alexander, Daniel C and Ravi, Daniele},
   title     = {DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification},
   booktitle = {Medical Imaging with Deep Learning},
   year      = {2023},
   selected  = {true},
   comment   = {MIDL 2023},
   preview   = {deepbrainprint-plot.png},
   pdf       = {deepbrainprint-article.pdf},
   poster    = {deepbrainprint-poster.pdf}, 
   code      = {https://github.com/DeepBrainPrint/DeepBrainPrint},
   abstract  = {Recent advances in MRI have led to the creation of large datasets. With the increase in data volume, it has become difficult to locate previous scans of the same patient within these datasets (a process known as re-identification). To address this issue, we propose an AI-powered medical imaging retrieval framework called DeepBrainPrint, which is designed to retrieve brain MRI scans of the same patient. Our framework is a semi-self-supervised contrastive deep learning approach with three main innovations. First, we use a combination of self-supervised and supervised paradigms to create an effective brain fingerprint from MRI scans that can be used for real-time image retrieval. Second, we use a special weighting function to guide the training and improve model convergence. Third, we introduce new imaging transformations to improve retrieval robustness in the presence of intensity variations (i.e. different scan contrasts), and to account for age and disease progression in patients. We tested DeepBrainPrint on a large dataset of T1-weighted brain MRIs from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and on a synthetic dataset designed to evaluate retrieval performance with different image modalities. Our results show that DeepBrainPrint outperforms previous methods, including simple similarity metrics and more advanced contrastive deep learning frameworks.},
}

@inproceedings{ravi2023efficient,
   author    = {Daniele Ravi and Frederik Barkhof and Daniel C. Alexander and Lemuel Puglisi and Geoffrey JM Parker and Arman Eshaghi},
   title     = {An efficient semi-supervised quality control system trained using physics-based MRI-artefact generators and adversarial training}, 
   booktitle = {Medical Image Analysis},
   year      = {2024},
   selected  = {false},
   comment   = {Medical Image Analysis},
   preview   = {automatic-qc-preview.png},
   pdf       = {https://www.sciencedirect.com/science/article/pii/S1361841523002931},
   code      = {https://github.com/Queen-Square-Analytics/automatic-quality-control},
   abstract  = {Large medical imaging data sets are becoming increasingly available, but ensuring sample quality without significant artefacts is challenging. Existing methods for identifying imperfections in medical imaging rely on data-intensive approaches, compounded by a scarcity of artefact-rich scans for training machine learning models in clinical research. To tackle this problem, we propose a framework with four main components: 1) artefact generators inspired by magnetic resonance physics to corrupt brain MRI scans and augment a training dataset, 2) abstract and engineered features to represent images compactly, 3) a feature selection process depending on the artefact class to improve classification, and 4) SVM classifiers to identify artefacts. Our contributions are threefold: first, physics-based artefact generators produce synthetic brain MRI scans with controlled artefacts for data augmentation. This will avoid the labour-intensive collection and labelling process of scans with rare artefacts. Second, we propose a pool of abstract and engineered image features to identify 9 different artefacts for structural MRI. Finally, we use an artefact-based feature selection block that, for each class of artefacts, finds the set of features providing the best classification performance. We performed validation experiments on a large data set of scans with artificially-generated artefacts, and in a multiple sclerosis clinical trial where real artefacts were identified by experts, showing that the proposed pipeline outperforms traditional methods. In particular, our data augmentation increases performance by up to 12.5 percentage points on accuracy, precision, and recall. The computational efficiency of our pipeline enables potential real-time deployment, promising high-throughput clinical applications through automated image-processing pipelines driven by quality control systems.},
   honor     = {<i>Impact Factor = 11.8</i>}
}